{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with OpenAI API for chunk 0: \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Type, TypeVar, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "import tiktoken\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Replace 'YOUR_API_KEY' with your actual OpenAI API\n",
    "T = TypeVar('T', bound=BaseModel)\n",
    "\n",
    "def extract_multi_needle(schema: Type[T], haystack: str, example_needles: List[str]) -> List[T]:\n",
    "    extracted_needles = []\n",
    "    model_name = 'gpt-4o-mini'\n",
    "\n",
    "    model = ChatOpenAI(\n",
    "        model_name=model_name,\n",
    "        temperature=0,\n",
    "        api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    "    )\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "\n",
    "    # Token limits\n",
    "    max_tokens_per_request = 16000\n",
    "    max_tokens_for_response = 1000\n",
    "    max_tokens_for_prompt = max_tokens_per_request - max_tokens_for_response\n",
    "\n",
    "    # Tokenize the haystack\n",
    "    haystack_tokens = encoding.encode(haystack)\n",
    "    num_tokens = len(haystack_tokens)\n",
    "\n",
    "    # Determine chunk size\n",
    "    chunk_size = 4000\n",
    "\n",
    "    # Split the haystack into chunks\n",
    "    chunks = [\n",
    "        encoding.decode(haystack_tokens[i:i + chunk_size])\n",
    "        for i in range(0, num_tokens, chunk_size)\n",
    "    ]\n",
    "\n",
    "    # Prepare schema description\n",
    "    schema_description = \"Extract information according to the following schema:\\n{\\n\"\n",
    "    for field_name, field in schema.__fields__.items():\n",
    "        field_descr = field.description or ''\n",
    "        field_type = (\n",
    "            field.annotation.__name__ if hasattr(field.annotation, '__name__') else str(field.annotation)\n",
    "        )\n",
    "        schema_description += f'  \"{field_name}\": \"{field_descr} ({field_type})\",\\n'\n",
    "    schema_description += \"}\\n\"\n",
    "\n",
    "    # Prepare examples\n",
    "    examples_text = \"Examples of the desired output format:\\n\"\n",
    "    for example in example_needles:\n",
    "        examples_text += f\"- {example}\\n\"\n",
    "\n",
    "    # System prompt\n",
    "    system_prompt = \"You are an AI language model that extracts structured data from text.\"\n",
    "\n",
    "    # Process each chunk\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        user_prompt = (\n",
    "            f\"{schema_description}\\n\"\n",
    "            f\"{examples_text}\\n\"\n",
    "            f\"Text to analyze:\\n\\\"\\\"\\\"\\n{chunk}\\n\\\"\\\"\\\"\\n\"\n",
    "            \"Extract any instances matching the schema from the text above. \"\n",
    "            \"Provide the output as a JSON array of objects.\"\n",
    "        )\n",
    "\n",
    "        # Ensure the prompt fits within token limits\n",
    "        prompt_tokens = encoding.encode(user_prompt)\n",
    "        if len(prompt_tokens) > max_tokens_for_prompt:\n",
    "            print(f\"Prompt too long for chunk {idx}, reducing chunk size.\")\n",
    "            continue\n",
    "\n",
    "        # Call OpenAI API\n",
    "        try:\n",
    "            response = model.invoke(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "            )\n",
    "            assistant_reply = response['choices'][0]['message']['content']\n",
    "\n",
    "            # Parse the assistant's reply\n",
    "            try:\n",
    "                extracted_data = json.loads(assistant_reply)\n",
    "                if isinstance(extracted_data, list):\n",
    "                    for item in extracted_data:\n",
    "                        try:\n",
    "                            extracted_item = schema(**item)\n",
    "                            extracted_needles.append(extracted_item)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error parsing item: {item}, error: {e}\")\n",
    "                else:\n",
    "                    print(f\"Expected a list, got: {type(extracted_data)}\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON decode error for chunk {idx}: {e}\")\n",
    "                print(\"Assistant reply:\")\n",
    "                print(assistant_reply)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with OpenAI API for chunk {idx}: {e}\")\n",
    "            if \"rate limit\" in str(e).lower():\n",
    "                print(\"Rate limit exceeded. Sleeping for 60 seconds.\")\n",
    "                time.sleep(60)\n",
    "                continue\n",
    "\n",
    "    return extracted_needles\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    class TechCompany(BaseModel):\n",
    "        name: Optional[str] = Field(default=None, description=\"The full name of the technology company\")\n",
    "        location: Optional[str] = Field(default=None, description=\"City and country where the company is headquartered\")\n",
    "        employee_count: Optional[int] = Field(default=None, description=\"Total number of employees\")\n",
    "        founding_year: Optional[int] = Field(default=None, description=\"Year the company was established\")\n",
    "        is_public: Optional[bool] = Field(default=None, description=\"Whether the company is publicly traded (True) or privately held (False)\")\n",
    "        valuation: Optional[float] = Field(default=None, description=\"Company's valuation in billions of dollars\")\n",
    "        primary_focus: Optional[str] = Field(default=None, description=\"Main area of technology or industry the company focuses on\")\n",
    "\n",
    "    example_needles = [\n",
    "        \"Ryoshi, based in Neo Tokyo, Japan, is a private quantum computing firm founded in 2031, currently valued at $8.7 billion with 1,200 employees focused on quantum cryptography.\"\n",
    "    ]\n",
    "\n",
    "    # Sample haystack text\n",
    "    haystack = \"\"\"\n",
    "    In the vast expanse of the universe, Ryoshi, based in Neo Tokyo, Japan, is a private quantum computing firm founded in 2031, currently valued at $8.7 billion with 1,200 employees focused on quantum cryptography.\n",
    "    Another notable entity is SandTech, located in Arrakeen, Arrakis, a public nano-engineering corporation established in 2045, with a valuation of $12.5 billion and employing 3,500 people, specializing in nano-filtration systems.\n",
    "    Amidst the dunes, SpiceCorp emerged in 2050, a private company headquartered in Sietch Tabr, employing 500 individuals dedicated to spice extraction technologies, currently valued at $2.1 billion.\n",
    "    \"\"\"\n",
    "\n",
    "    extracted_data = extract_multi_needle(TechCompany, haystack, example_needles)\n",
    "\n",
    "    for item in extracted_data:\n",
    "        print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
