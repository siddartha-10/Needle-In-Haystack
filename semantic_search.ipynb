{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import List, Type, TypeVar\n",
    "from pydantic import BaseModel\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Define a generic type variable bound to BaseModel\n",
    "T = TypeVar('T', bound=BaseModel)\n",
    "\n",
    "def compute_embeddings_multithreaded(texts: List[str], model_name: str = 'all-MiniLM-L6-v2', batch_size: int = 64) -> List:\n",
    "    \"\"\"\n",
    "    Computes embeddings for a list of texts using multi-threading.\n",
    "\n",
    "    Args:\n",
    "        texts (List[str]): The list of texts to compute embeddings for.\n",
    "        model_name (str): The name of the SentenceTransformer model to use.\n",
    "        batch_size (int): The batch size for embedding computation.\n",
    "\n",
    "    Returns:\n",
    "        List: A list of embeddings corresponding to the input texts.\n",
    "    \"\"\"\n",
    "    # Initialize the embedding model\n",
    "    embedding_model = SentenceTransformer(model_name)\n",
    "\n",
    "    # Function to compute embeddings for a batch of texts\n",
    "    def compute_batch(batch_texts):\n",
    "        return embedding_model.encode(batch_texts, show_progress_bar=False)\n",
    "\n",
    "    embeddings = [None] * len(texts)\n",
    "\n",
    "    # Create batches of texts\n",
    "    batches = [(i, texts[i:i + batch_size]) for i in range(0, len(texts), batch_size)]\n",
    "\n",
    "    # Use ThreadPoolExecutor for multi-threading\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # Submit tasks\n",
    "        future_to_batch = {executor.submit(compute_batch, batch_texts): (i, batch_texts) for i, batch_texts in batches}\n",
    "        # Collect results as they complete\n",
    "        for future in as_completed(future_to_batch):\n",
    "            i, batch_texts = future_to_batch[future]\n",
    "            try:\n",
    "                batch_embeddings = future.result()\n",
    "                embeddings[i:i + len(batch_embeddings)] = batch_embeddings\n",
    "            except Exception as exc:\n",
    "                print(f'Batch starting at index {i} generated an exception: {exc}')\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def extract_multi_needle(schema: Type[T], haystack: str, example_needles: List[str]) -> List[T]:\n",
    "    \"\"\"\n",
    "    Extracts and structures information from a large text corpus based on a given schema and examples.\n",
    "\n",
    "    Args:\n",
    "        schema (Type[T]): A Pydantic model defining the structure of the needle to be extracted.\n",
    "        haystack (str): The large text corpus to search through (haystack).\n",
    "        example_needles (List[str]): A list of example sentences (needles).\n",
    "\n",
    "    Returns:\n",
    "        List[T]: A list of extracted needles conforming to the provided schema.\n",
    "    \"\"\"\n",
    "    # Initialize the list to hold the extracted needles\n",
    "    extracted_needles = []\n",
    "\n",
    "    # Initialize the SentenceTransformer model name\n",
    "    model_name = 'all-MiniLM-L6-v2'\n",
    "\n",
    "    # Split the haystack into sentences using regex\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', haystack)\n",
    "\n",
    "    # Compute embeddings for the sentences in the haystack using multi-threading\n",
    "    sentence_embeddings = compute_embeddings_multithreaded(sentences, model_name=model_name)\n",
    "\n",
    "    # Compute embeddings for the example needles\n",
    "    embedding_model = SentenceTransformer(model_name)\n",
    "    example_embeddings = embedding_model.encode(example_needles, show_progress_bar=True)\n",
    "\n",
    "    # Build a Faiss index for efficient similarity search\n",
    "    dimension = len(sentence_embeddings[0])\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    # Convert embeddings to numpy array\n",
    "    import numpy as np\n",
    "    sentence_embeddings = np.array(sentence_embeddings)\n",
    "    faiss.normalize_L2(sentence_embeddings)\n",
    "    index.add(sentence_embeddings)\n",
    "\n",
    "    # Normalize example embeddings for cosine similarity\n",
    "    example_embeddings = np.array(example_embeddings)\n",
    "    faiss.normalize_L2(example_embeddings)\n",
    "\n",
    "    # Number of nearest neighbors to retrieve\n",
    "    k = 5\n",
    "\n",
    "    # Perform similarity search for each example embedding\n",
    "    D, I = index.search(example_embeddings, k)\n",
    "\n",
    "    # Collect candidate sentences based on similarity search\n",
    "    candidate_sentences = set()\n",
    "    for indices in I:\n",
    "        for idx in indices:\n",
    "            candidate_sentences.add(sentences[idx])\n",
    "\n",
    "    # Convert the set to a list for processing\n",
    "    candidate_sentences = list(candidate_sentences)\n",
    "\n",
    "    # Initialize the Azure OpenAI LLM model\n",
    "    model = AzureChatOpenAI(\n",
    "        openai_api_version=os.environ.get(\"AZURE_OPENAI_VERSION\", \"2024-07-18\"),\n",
    "        azure_deployment=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\", \"gpt-4o-mini\"),\n",
    "        azure_endpoint=os.environ.get(\n",
    "            \"AZURE_OPENAI_ENDPOINT\",\n",
    "            \"https://gptmini4o.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2023-03-15-preview\"\n",
    "        ),\n",
    "        openai_api_key=os.environ.get(\"AZURE_OPENAI_KEY\", \"your_default_api_key_here\"),\n",
    "    )\n",
    "\n",
    "    # Generate a description of the schema fields and their descriptions\n",
    "    def generate_schema_description(schema: Type[BaseModel]) -> str:\n",
    "        \"\"\"\n",
    "        Generates a string description of the schema.\n",
    "\n",
    "        Args:\n",
    "            schema (Type[BaseModel]): The Pydantic model.\n",
    "\n",
    "        Returns:\n",
    "            str: A string describing the schema fields and their descriptions.\n",
    "        \"\"\"\n",
    "        schema_description = \"\"\n",
    "        for field_name, field in schema.__fields__.items():\n",
    "            field_desc = field.field_info.description or ''\n",
    "            field_type = field.outer_type_.__name__\n",
    "            schema_description += f\"- {field_name} ({field_type}): {field_desc}\\n\"\n",
    "        return schema_description\n",
    "\n",
    "    schema_description = generate_schema_description(schema)\n",
    "\n",
    "    # Construct the system prompt with schema description\n",
    "    system_prompt = f\"\"\"\n",
    "You are an assistant that extracts information from text according to a given schema.\n",
    "\n",
    "The schema is:\n",
    "{schema_description}\n",
    "\n",
    "Your task is to read the provided text and extract any information that matches the schema.\n",
    "\n",
    "Provide the extracted data as a JSON object conforming to the schema.\n",
    "\n",
    "If the text does not contain relevant information, output an empty JSON object.\n",
    "\n",
    "Only provide the JSON object, and no additional text.\n",
    "\"\"\"\n",
    "\n",
    "    # Process each candidate sentence\n",
    "    for text in candidate_sentences:\n",
    "        # Create the conversation messages for the LLM\n",
    "        messages = [\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(content=text)\n",
    "        ]\n",
    "\n",
    "        # Call the LLM to process the text\n",
    "        response = model(messages)\n",
    "\n",
    "        # Attempt to parse the LLM response as JSON\n",
    "        try:\n",
    "            data = json.loads(response.content)\n",
    "            if data:  # If data is not empty\n",
    "                # Validate and instantiate the schema\n",
    "                item = schema(**data)\n",
    "                extracted_needles.append(item)\n",
    "        except json.JSONDecodeError:\n",
    "            # If parsing fails, skip this text\n",
    "            continue\n",
    "        except Exception:\n",
    "            # If data does not conform to schema, skip\n",
    "            continue\n",
    "\n",
    "    return extracted_needles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class TechCompany(BaseModel):\n",
    "    name: Optional[str] = Field(default=None, description=\"The full name of the technology company\")\n",
    "    location: Optional[str] = Field(default=None, description=\"City and country where the company is headquartered\")\n",
    "    employee_count: Optional[int] = Field(default=None, description=\"Total number of employees\")\n",
    "    founding_year: Optional[int] = Field(default=None, description=\"Year the company was established\")\n",
    "    is_public: Optional[bool] = Field(default=None, description=\"Whether the company is publicly traded (True) or privately held (False)\")\n",
    "    valuation: Optional[float] = Field(default=None, description=\"Company's valuation in billions of dollars\")\n",
    "    primary_focus: Optional[str] = Field(default=None, description=\"Main area of technology or industry the company focuses on\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_needles = [\"Ryoshi, based in Neo Tokyo, Japan, is a private quantum computing firm founded in 2031, currently valued at $8.7 billion with 1,200 employees focused on quantum cryptography.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"haystack.txt\", \"r\") as file:\n",
    "    haystack_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "extracted_data = extract_multi_needle(schema=TechCompany, haystack=haystack_text, example_needles=example_needles)\n",
    "\n",
    "# Serialize the extracted data to a JSON file\n",
    "with open('extracted_needles.json', 'w') as f:\n",
    "    json.dump([item.dict() for item in extracted_data], f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
