{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in /Users/siddartha/miniforge3/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from sentence_transformers) (4.45.2)\n",
      "Requirement already satisfied: tqdm in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from sentence_transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from sentence_transformers) (2.4.0)\n",
      "Requirement already satisfied: numpy in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: scipy in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from sentence_transformers) (1.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from sentence_transformers) (0.24.7)\n",
      "Requirement already satisfied: Pillow in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from sentence_transformers) (10.3.0)\n",
      "Requirement already satisfied: filelock in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (69.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.20.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from scikit-learn->sentence_transformers) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: langchain in /Users/siddartha/miniforge3/lib/python3.12/site-packages (0.3.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from langchain) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from langchain) (3.10.5)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.10 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from langchain) (0.3.10)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from langchain) (0.1.134)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from langchain) (2.7.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from langchain) (8.4.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.5)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.18.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
      "Requirement already satisfied: anyio in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/siddartha/miniforge3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.10->langchain) (2.1)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers\n",
    "!pip install langchain\n",
    "!pip install faiss-gpu langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from typing import Type, List, TypeVar\n",
    "from pydantic import BaseModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from tqdm import tqdm  # For progress bar visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = TypeVar('T', bound=BaseModel)\n",
    "\n",
    "def extract_multi_needle(schema: Type[T], haystack: str, example_needles: List[str]) -> List[T]:\n",
    "    \"\"\"\n",
    "    Extracts information from a large text (haystack) based on example sentences (needles)\n",
    "    and a defined schema. Returns a list of extracted data conforming to the schema.\n",
    "    \"\"\"\n",
    "    # Initialize the list to hold the extracted data\n",
    "    extracted_needles = []\n",
    "    \n",
    "    # Initialize the SentenceTransformer model for embeddings (fast and efficient)\n",
    "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Split the haystack into individual sentences using regex\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', haystack)\n",
    "    \n",
    "    # Compute embeddings for the sentences in the haystack\n",
    "    sentence_embeddings = embedding_model.encode(\n",
    "        sentences, batch_size=256, show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Compute embeddings for the example needles\n",
    "    example_embeddings = embedding_model.encode(\n",
    "        example_needles, batch_size=256, show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Normalize embeddings to unit vectors for cosine similarity calculation\n",
    "    sentence_embeddings_normalized = sentence_embeddings / np.linalg.norm(sentence_embeddings, axis=1, keepdims=True)\n",
    "    example_embeddings_normalized = example_embeddings / np.linalg.norm(example_embeddings, axis=1, keepdims=True)\n",
    "    \n",
    "    # Compute cosine similarities between example needles and sentences\n",
    "    cosine_similarities = np.dot(example_embeddings_normalized, sentence_embeddings_normalized.T)\n",
    "    \n",
    "    # Set a similarity threshold to select relevant sentences\n",
    "    similarity_threshold = 0.4  # Adjust this value as needed\n",
    "    \n",
    "    # Get indices of sentences that have similarity above the threshold\n",
    "    candidate_indices = np.argwhere(cosine_similarities >= similarity_threshold)[:, 1]\n",
    "    \n",
    "    # Retrieve the candidate sentences based on the indices\n",
    "    candidate_sentences = [sentences[idx] for idx in set(candidate_indices)]\n",
    "    \n",
    "    # Initialize the Azure OpenAI LLM model\n",
    "    model = AzureChatOpenAI(\n",
    "        openai_api_version=os.environ.get(\"AZURE_OPENAI_VERSION\", \"2023-03-15-preview\"),\n",
    "        azure_deployment=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\", \"gpt-4o-mini\"),\n",
    "        azure_endpoint=os.environ.get(\n",
    "            \"AZURE_OPENAI_ENDPOINT\",\n",
    "            \"https://gptmini4o.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2023-03-15-preview\"\n",
    "        ),\n",
    "        openai_api_key=os.environ.get(\"AZURE_OPENAI_KEY\", \"c2105be0c2744742980b57320b87e813\"),\n",
    "    )\n",
    "    \n",
    "    # Generate a description of the schema to include in the prompts\n",
    "    schema_description = generate_schema_description(schema)\n",
    "    \n",
    "    # Generate keywords using the LLM\n",
    "    keywords = generate_keywords(example_needles, schema_description, model)\n",
    "    \n",
    "    # Include sentences that contain any of the generated keywords\n",
    "    keyword_sentences = [\n",
    "        sentence for sentence in sentences \n",
    "        if any(keyword.lower() in sentence.lower() for keyword in keywords)\n",
    "    ]\n",
    "    \n",
    "    # Combine the candidate sentences from embeddings and keyword matching\n",
    "    candidate_sentences = list(set(candidate_sentences).union(set(keyword_sentences)))\n",
    "    \n",
    "    print(f\"Number of candidate sentences: {len(candidate_sentences)}\")\n",
    "    \n",
    "    # Construct the system prompt with schema description for the LLM\n",
    "    system_prompt = f\"\"\"\n",
    "You are an assistant that extracts information from text according to a given schema.\n",
    "\n",
    "The schema is:\n",
    "{schema_description}\n",
    "\n",
    "Your task is to read the provided text and extract any information that matches the schema.\n",
    "\n",
    "Provide the extracted data as a JSON object conforming to the schema.\n",
    "\n",
    "If the text does not contain relevant information, output an empty JSON object.\n",
    "\n",
    "Only provide the JSON object, and no additional text.\n",
    "\n",
    "Consider variations in sentence structure and wording. Extract information even if the text differs from the examples.\n",
    "\"\"\"\n",
    "    \n",
    "    # Process the candidate sentences in parallel using threading\n",
    "    extracted_needles = process_sentences_in_parallel(candidate_sentences, system_prompt, model, schema)\n",
    "    \n",
    "    return extracted_needles\n",
    "\n",
    "def generate_schema_description(schema: Type[BaseModel]) -> str:\n",
    "    \"\"\"\n",
    "    Generates a text description of the schema fields and their types.\n",
    "    \"\"\"\n",
    "    schema_description = \"\"\n",
    "    for field_name, field in schema.__fields__.items():\n",
    "        field_desc = field.description or ''\n",
    "        field_type = (\n",
    "              field.annotation.__name__ if hasattr(field.annotation, '__name__') else str(field.annotation)\n",
    "          )\n",
    "        schema_description += f\"- {field_name} ({field_type}): {field_desc}\\n\"\n",
    "    return schema_description\n",
    "\n",
    "def generate_keywords(example_needles: List[str], schema_description: str, model) -> List[str]:\n",
    "    \"\"\"\n",
    "    Uses the LLM to generate a list of keywords based on the example needles and schema.\n",
    "    \"\"\"\n",
    "    # Construct a prompt for the LLM\n",
    "    prompt = f\"\"\"Given the following schema and example needles, generate a list of keywords that would be useful for identifying relevant sentences in a text. The keywords should be related to the schema fields and the type of information we're looking for.\n",
    "\n",
    "Schema:\n",
    "{schema_description}\n",
    "\n",
    "Example needles:\n",
    "{', '.join(example_needles)}\n",
    "\n",
    "Please provide a comma-separated list of approximately 10 keywords. These keywords should be close words, i.e., there are needles present in the haystack which are structurally very similar to the example needles. Choose keywords that will help identify these other similar needles as well.\"\"\"\n",
    "    # Create the conversation messages for the LLM\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are an assistant that extracts keywords from text.\"),\n",
    "        HumanMessage(content=prompt)\n",
    "    ]\n",
    "    \n",
    "    # Call the LLM to generate keywords\n",
    "    response = model.invoke(messages, temperature=0.2)\n",
    "    \n",
    "    # Parse the response to extract keywords\n",
    "    keywords_text = response.content.strip()\n",
    "    keywords = [kw.strip() for kw in keywords_text.split(',') if kw.strip()]\n",
    "    return keywords\n",
    "\n",
    "def process_sentences_in_parallel(candidate_sentences: List[str], system_prompt: str, model, schema: Type[T]) -> List[T]:\n",
    "    \"\"\"\n",
    "    Processes multiple sentences in parallel using threading to make API calls concurrently.\n",
    "    Returns a list of extracted data conforming to the schema.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    error_indices = []\n",
    "\n",
    "    # Use ThreadPoolExecutor to process sentences concurrently\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Submit tasks to the executor\n",
    "        future_to_index = {\n",
    "            executor.submit(process_sentence, index, text, system_prompt, model, schema): index\n",
    "            for index, text in enumerate(candidate_sentences)\n",
    "        }\n",
    "        \n",
    "        # Process the futures as they complete\n",
    "        for future in tqdm(concurrent.futures.as_completed(future_to_index),\n",
    "                           total=len(candidate_sentences), desc=\"Processing Sentences\"):\n",
    "            index = future_to_index[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results[index] = result\n",
    "            except Exception as exc:\n",
    "                print(f'Sentence {index} generated an exception: {exc}')\n",
    "                results[index] = None\n",
    "                error_indices.append(index)\n",
    "    \n",
    "    # Collect the extracted items from results\n",
    "    extracted_items = [result for result in results.values() if result is not None]\n",
    "    \n",
    "    return extracted_items\n",
    "\n",
    "def process_sentence(index: int, text: str, system_prompt: str, model, schema: Type[T]) -> T:\n",
    "    \"\"\"\n",
    "    Processes a single sentence using the LLM to extract information according to the schema.\n",
    "    Returns an instance of the schema if data is extracted, or None otherwise.\n",
    "    \"\"\"\n",
    "    # Create the conversation messages for the LLM\n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=text)\n",
    "    ]\n",
    "    \n",
    "    # Call the LLM to process the text\n",
    "    response = model.invoke(messages)\n",
    "    \n",
    "    # Attempt to parse the LLM response as JSON\n",
    "    try:\n",
    "        data = json.loads(response.content)\n",
    "        if data:  # If data is not empty\n",
    "            # Validate and instantiate the schema with the extracted data\n",
    "            item = schema(**data)\n",
    "            return item\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"JSONDecodeError for sentence {index}: {text}\")\n",
    "        print(f\"LLM response: {response.content}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Exception for sentence {index}: {text}\")\n",
    "        print(f\"Error: {e}\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class TechCompany(BaseModel):\n",
    "    name: Optional[str] = Field(default=None, description=\"The full name of the technology company\")\n",
    "    location: Optional[str] = Field(default=None, description=\"City and country where the company is headquartered\")\n",
    "    employee_count: Optional[int] = Field(default=None, description=\"Total number of employees\")\n",
    "    founding_year: Optional[int] = Field(default=None, description=\"Year the company was established\")\n",
    "    is_public: Optional[bool] = Field(default=None, description=\"Whether the company is publicly traded (True) or privately held (False)\")\n",
    "    valuation: Optional[float] = Field(default=None, description=\"Company's valuation in billions of dollars\")\n",
    "    primary_focus: Optional[str] = Field(default=None, description=\"Main area of technology or industry the company focuses on\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_needles = [\"Ryoshi, based in Neo Tokyo, Japan, is a private quantum computing firm founded in 2031, currently valued at $8.7 billion with 1,200 employees focused on quantum cryptography.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"haystack.txt\", \"r\") as file:\n",
    "    haystack_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e273a9bbd846407289a76a93429c6875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1070 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e412a17f298423e9405e90c626a172e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidate sentences: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences: 100%|██████████| 16/16 [00:01<00:00, 15.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "extracted_data = extract_multi_needle(schema=TechCompany, haystack=haystack_text, example_needles=example_needles)\n",
    "\n",
    "# Serialize the extracted data to a JSON file\n",
    "with open('extracted_needles.json', 'w') as f:\n",
    "    json.dump([item.dict() for item in extracted_data], f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_csv(json_file: str, csv_file: str):\n",
    "    \"\"\"\n",
    "    Converts a JSON file to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        json_file (str): The path to the input JSON file.\n",
    "        csv_file (str): The path to the output CSV file.\n",
    "    \"\"\"\n",
    "    # Check if the JSON file exists\n",
    "    if not os.path.exists(json_file):\n",
    "        print(f\"File {json_file} not found.\")\n",
    "        return\n",
    "\n",
    "    # Read the JSON data\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # If the JSON data is a dictionary, convert it to a list of dictionaries\n",
    "    if isinstance(data, dict):\n",
    "        data = [data]\n",
    "\n",
    "    # Get the keys (column names) from the first item\n",
    "    fieldnames = data[0].keys()\n",
    "\n",
    "    # Write data to the CSV file\n",
    "    with open(csv_file, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "    print(f\"CSV file saved as {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved as needles.csv\n",
      "Extracted data saved to needles.csv\n"
     ]
    }
   ],
   "source": [
    "csv_file = \"needles.csv\"\n",
    "json_to_csv(\"extracted_needles.json\", csv_file)\n",
    "print(f\"Extracted data saved to {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
